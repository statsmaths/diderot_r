---
title: "Handout 11: NLP with cleanNLP"
date: Taylor Arnold
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
---


```{r global_options, include=FALSE}
library(tufte)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(tidy = FALSE)

library(tidyverse)
library(smodels)
library(forcats)
library(ggrepel)
library(stringr)
library(Matrix)

library(sotu)
library(cleanNLP)
library(magrittr)
data(sotu_text)
data(sotu_meta)

sotu <- readRDS("~/files/sotu_data/sotu_cleanNLP.rds")

theme_set(theme_minimal())
msleep <- na.omit(msleep)
```

\justify

## Loading and parsing the data

The full text of all the State of the Union addresses through 2016 are
available in the R package **sotu**, available
on CRAN. The package also contains meta-data concerning each speech
that we will add to the document table while annotating the corpus.
The code to run this annotation is given by:
```{r, eval = FALSE}
library(sotu)
library(cleanNLP)

data(sotu_text)
data(sotu_meta)
init_spaCy()
sotu <- cleanNLP::run_annotators(sotu_text, as_strings = TRUE,
                                 meta = sotu_meta)
```
The annotation object, which we will use in the example in the following
analysis, is stored in the object `sotu`. We will create a single
data frame with all of the tokens here and save the metadata
as a seperate data set:

```{r}
tokens <- cleanNLP::get_token(sotu, combine = TRUE)
tokens
doc <- cleanNLP::get_document(sotu)
```

## Exploratory analysis


Simple summary statistics are easily computed off of the token table. To see
the distribution of sentence length, the token table is grouped by the document
and sentence id and the number of rows within each group are computed. The percentiles
of these counts give a quick summary of the distribution.
```{r, eval = TRUE}
library(ggplot2)
library(dplyr)
temp <- count(tokens, id, sid)
quantile(temp$n, seq(0,1,0.1))
```
The median sentence has 28 tokens, whereas at least one has over 600 (this is
due to a bulleted list in one of the written addresses being treated as a single sentence)
To see the most frequently used
nouns in the dataset, the token table is filtered on the universal part of speech field, grouped
by lemma, and the number of rows in each group are once again calculated. Sorting the output and
selecting the top $42$ nouns, yields a high level summary of the topics of interest within this
corpus.
```{r, eval = TRUE}
temp <- filter(tokens, upos == "NOUN")
temp <- count(temp, lemma)
temp <- top_n(temp, n = 42, n)
arrange(temp, desc(n))$lemma
```
The result is generally as would be expected from a corpus of government speeches, with
references to proper nouns representing various organizations within the government and
non-proper nouns indicating general topics of interest such as ``tax'', ``law'', and
``peace''.

The length in tokens of each address is calculated similarly by grouping and summarizing at
the document id level. The results can be joined with the document table to get the year
of the speech and then piped in a **ggplot2** command to illustrate how the length of
the State of the Union has changed over time.
```{r, eval = FALSE}
doc <- cleanNLP::get_document(sotu)
temp <- left_join(count(tokens, id), doc)

qplot(year, n, data = temp, color = sotu_type) +
  geom_line() +
  geom_smooth()
```
Here, color is used to represent whether the address was given as an oral address or a written
document. The output shows that their are certainly time trends
to the address length, with the form of the address (written versus spoken) also having a large
effect on document length.

```{r, results = "asis", echo = FALSE, out.width = "100%", fig.fullwidth = TRUE, eval = TRUE, fig.cap = "Length of each State of the Union address, in total number of tokens. Color shows whether the address was given as a speech or delivered as a written document."}
knitr::include_graphics("img/num_tokens.pdf")
```

Finding the most used entities from the entity table over the time period of the corpus yields an
alternative way to see the underlying topics. A slightly modified version of the code
snippet used to find the top nouns in the dataset can be used to find the top entities.
The `get_token` function is replaced by `get_entity` and the table is filtered
on `entity_type` rather than the universal part of speech code.
```{r, eval = TRUE}
temp <- filter(tokens, entity_type == "GPE")
temp <- count(temp, entity)
temp <- top_n(temp, n = 26, n)
arrange(temp, desc(n))$entity
```
The ability to redo analyses from a slightly different perspective is a direct consequence of
the tidy data model supplied by **cleanNLP**.
The top locations include some obvious and some less obvious instances.
Those sovereign nations included such as Great Britain, Mexico, Germany, and Japan seem
as expected given either the United State's close ties or periods of war with them. The top states
include the most populous regions (New York, California, and Texas) but also smaller
states (Kansas, Oregon, Mississippi), the latter being more surprising.

One of the most straightforward way of extracting a high-level summary of the content of a speech
is to extract all direct object object dependencies where the target noun is not a very common word.
In order to do this for a particular speech, the dependency table is joined to the document table,
a particular document is selected, and relationships of type ``dobj'' (direct object)
are filtered out. The result is then joined to the data set `word_frequency`, which is
included with **cleanNLP**, and pairs with a target occurring less than 0.5\% of the time
are selected to give the final result. Here is an example of this using the first address made
by George W. Bush in 2001:
```{r, eval = TRUE}
temp <- left_join(tokens, doc)
temp <- filter(temp, year == 2001, relation == "dobj")
temp <- select(temp, id = id, start = word, word = lemma_source)
temp <- left_join(temp, word_frequency)
temp <- filter(temp, frequency < 0.001)
temp <- select(temp, id, start, word)
sprintf("%s => %s", temp$start, temp$word)
```
Most of these phrases correspond with the ``compassionate conservatism" that George W. Bush ran
under in the preceding 2000 election. Applying the same analysis to the 2002 State of the Union,
which came under the shadow of the September 11th terrorist attacks, shows a drastic shift
in focus.
```{r, eval = TRUE}
temp <- left_join(tokens, doc)
temp <- filter(temp, year == 2001, relation == "dobj")
temp <- select(temp, id = id, start = word, word = lemma_source)
temp <- left_join(temp, word_frequency)
temp <- filter(temp, frequency < 0.0005)
temp <- select(temp, id, start, word)
sprintf("%s => %s", temp$start, temp$word)
```
Here the topics have almost entirely shifted to counter-terrorism and national security efforts.
